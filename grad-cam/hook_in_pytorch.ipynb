{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xS0y4ibsnBO1"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, Module\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc_1 = Linear(10, 20)\n",
        "        self.fc_2 = Linear(20, 30)\n",
        "        self.fc_3 = Linear(30, 2)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc_1(x)\n",
        "        x = self.fc_2(x)\n",
        "        x = self.fc_3(x)\n",
        "\n",
        "        x = F.relu(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "DYti_VfJ6eNI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hook :  You can register a hook on a Tensor or a nn.Module. A hook is basically a function that is executed when the either forward or backward is called."
      ],
      "metadata": {
        "id": "mnU6ynRpX3RX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch provides two types of hooks.\n",
        "\n",
        "    The Forward Hook\n",
        "    The Backward Hook\n",
        "\n",
        "A forward hook is executed during the forward pass, while the backward hook is , well, you guessed it, executed when the backward function is called. Time to remind you again, these are the forward and backward functions of an Autograd.Function object.\n",
        "\n",
        "for forward hook: hook(module, input, output) -> None \\\\\n",
        "for backward hook:  hook(module, grad_input, grad_output) -> Tensor or None  \\\\\n",
        "\n",
        "grad_input is the gradient of the input of nn.Module object w.r.t to the loss . grad_output is the gradient of the output of the nn.Module object w.r.t to the loss. "
      ],
      "metadata": {
        "id": "Y1HYpT6QYWEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    log_dir = pathlib.Path.cwd() / \"tensorboard_logs\"\n",
        "    writer = SummaryWriter(log_dir)\n",
        "\n",
        "    x = torch.rand(1, 10)\n",
        "    net = Network()\n",
        "\n",
        "    def activation_hook(inst, inp, out):\n",
        "        \"\"\"Run activation hook.\n",
        "        Parameters\n",
        "        ----------\n",
        "        inst : torch.nn.Module\n",
        "            The layer we want to attach the hook to.\n",
        "        inp : tuple of torch.Tensor\n",
        "            The input to the `forward` method.\n",
        "        out : torch.Tensor\n",
        "            The output of the `forward` method.\n",
        "        \"\"\"\n",
        "        print(repr(inst))\n",
        "        print(out)\n",
        "        #writer.add_histogram(repr(inst), out)\n",
        "\n",
        "    handle_1 = net.fc_1.register_forward_hook(activation_hook)\n",
        "    # net.fc_2.register_forward_hook(activation_hook)\n",
        "    # net.fc_3.register_forward_hook(activation_hook)\n",
        "\n",
        "    #y = net(x)\n",
        "    #handle_1.remove()\n",
        "    y = net(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_qzYS0u6m6A",
        "outputId": "c5a4d50f-2db0-4a74-9143-a780712431a7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=10, out_features=20, bias=True)\n",
            "tensor([[ 0.2694,  0.0410,  0.3110,  0.7013, -0.2220,  0.2695, -0.2852,  0.2360,\n",
            "         -0.6568,  0.7898,  0.3506, -0.4706,  0.5300,  0.4436, -0.2577,  0.2203,\n",
            "         -0.5060,  0.5891, -0.4182, -0.5218]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !tensorboard --logdir=tensorboard_logs/"
      ],
      "metadata": {
        "id": "fgIr2YPC7FWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bFT1s1CBaP8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "\n",
        "class myNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Conv2d(3,10,2, stride = 2)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.flatten = lambda x: x.view(-1)\n",
        "    self.fc1 = nn.Linear(160,5)\n",
        "   \n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.relu(self.conv(x))\n",
        "    return self.fc1(self.flatten(x))\n",
        "  \n",
        "\n",
        "net = myNet()\n",
        "\n",
        "def hook_fn(m, i, o):\n",
        "  print(m)\n",
        "  print(\"------------Input Grad------------\")\n",
        "\n",
        "  for grad in i:\n",
        "    try:\n",
        "      print(grad)\n",
        "    except AttributeError: \n",
        "      print (\"None found for Gradient\")\n",
        "\n",
        "  print(\"------------Output Grad------------\")\n",
        "  for grad in o:  \n",
        "    try:\n",
        "      print(grad)\n",
        "    except AttributeError: \n",
        "      print (\"None found for Gradient\")\n",
        "  print(\"\\n\")\n",
        "\n",
        "  \n",
        "net.conv.register_backward_hook(hook_fn)\n",
        "net.fc1.register_backward_hook(hook_fn)\n",
        "inp = torch.randn(1,3,8,8)\n",
        "out = net(inp)\n",
        "\n",
        "(1 - out.mean()).backward()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUJJASk1aPqV",
        "outputId": "1b56d4ab-4b38-4518-c255-85a116eed25d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=160, out_features=5, bias=True)\n",
            "------------Input Grad------------\n",
            "tensor([-0.2000, -0.2000, -0.2000, -0.2000, -0.2000])\n",
            "tensor([-0.2000, -0.2000, -0.2000, -0.2000, -0.2000])\n",
            "------------Output Grad------------\n",
            "tensor([-0.2000, -0.2000, -0.2000, -0.2000, -0.2000])\n",
            "\n",
            "\n",
            "Conv2d(3, 10, kernel_size=(2, 2), stride=(2, 2))\n",
            "------------Input Grad------------\n",
            "None\n",
            "tensor([[[[-0.0516,  0.0196],\n",
            "          [ 0.0454, -0.0095]],\n",
            "\n",
            "         [[ 0.0678, -0.0408],\n",
            "          [-0.0185, -0.0377]],\n",
            "\n",
            "         [[ 0.0136,  0.0045],\n",
            "          [ 0.0091, -0.0008]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0232, -0.0197],\n",
            "          [-0.0082,  0.1146]],\n",
            "\n",
            "         [[-0.0988, -0.0600],\n",
            "          [ 0.0197, -0.0305]],\n",
            "\n",
            "         [[-0.0744,  0.0460],\n",
            "          [-0.0404,  0.0729]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0126, -0.0383],\n",
            "          [-0.0209,  0.0138]],\n",
            "\n",
            "         [[-0.0587,  0.0298],\n",
            "          [ 0.0336,  0.0232]],\n",
            "\n",
            "         [[ 0.0148, -0.0101],\n",
            "          [ 0.0293, -0.0435]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0422,  0.0521],\n",
            "          [ 0.0440,  0.0395]],\n",
            "\n",
            "         [[-0.0544,  0.0729],\n",
            "          [ 0.0351,  0.0334]],\n",
            "\n",
            "         [[-0.0045, -0.0177],\n",
            "          [ 0.1039, -0.0264]]],\n",
            "\n",
            "\n",
            "        [[[-0.0018, -0.0668],\n",
            "          [ 0.0520,  0.0217]],\n",
            "\n",
            "         [[ 0.0265, -0.0073],\n",
            "          [ 0.0345, -0.0344]],\n",
            "\n",
            "         [[ 0.0148, -0.0148],\n",
            "          [-0.0328,  0.0424]]],\n",
            "\n",
            "\n",
            "        [[[-0.0353,  0.0132],\n",
            "          [-0.0229,  0.0740]],\n",
            "\n",
            "         [[-0.2036,  0.0388],\n",
            "          [ 0.0942,  0.0180]],\n",
            "\n",
            "         [[-0.0434,  0.0351],\n",
            "          [ 0.0783, -0.0339]]],\n",
            "\n",
            "\n",
            "        [[[-0.0404,  0.0366],\n",
            "          [ 0.0249,  0.0310]],\n",
            "\n",
            "         [[-0.0489, -0.0057],\n",
            "          [-0.0195, -0.0411]],\n",
            "\n",
            "         [[-0.0179,  0.1231],\n",
            "          [ 0.0239,  0.0103]]],\n",
            "\n",
            "\n",
            "        [[[-0.0352, -0.0136],\n",
            "          [-0.0080,  0.0424]],\n",
            "\n",
            "         [[-0.1184,  0.0356],\n",
            "          [ 0.0214, -0.0230]],\n",
            "\n",
            "         [[-0.0039,  0.1043],\n",
            "          [ 0.0016, -0.0032]]],\n",
            "\n",
            "\n",
            "        [[[-0.1460, -0.0221],\n",
            "          [-0.0117, -0.1443]],\n",
            "\n",
            "         [[ 0.0752,  0.0755],\n",
            "          [ 0.0093,  0.0739]],\n",
            "\n",
            "         [[-0.0949, -0.1115],\n",
            "          [ 0.0463, -0.0361]]],\n",
            "\n",
            "\n",
            "        [[[-0.0003, -0.0365],\n",
            "          [-0.0895, -0.0586]],\n",
            "\n",
            "         [[-0.0008, -0.0075],\n",
            "          [-0.0167,  0.0374]],\n",
            "\n",
            "         [[ 0.0163, -0.0664],\n",
            "          [-0.0809, -0.0083]]]])\n",
            "tensor([ 0.0024, -0.0110, -0.0465,  0.0203, -0.0701, -0.0053,  0.0159,  0.0098,\n",
            "         0.0126,  0.0592])\n",
            "------------Output Grad------------\n",
            "tensor([[[[ 0.0000,  0.0000, -0.0005,  0.0140],\n",
            "          [ 0.0000,  0.0159,  0.0000,  0.0000],\n",
            "          [-0.0269, -0.0002,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0165,  0.0000,  0.0000, -0.0288],\n",
            "          [ 0.0121, -0.0140, -0.0160, -0.0078],\n",
            "          [ 0.0084,  0.0039,  0.0000,  0.0214],\n",
            "          [ 0.0094,  0.0000,  0.0000, -0.0161]],\n",
            "\n",
            "         [[-0.0114, -0.0003,  0.0000,  0.0000],\n",
            "          [ 0.0060, -0.0102, -0.0104, -0.0013],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [-0.0126,  0.0000, -0.0064,  0.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0000, -0.0123,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0360, -0.0301,  0.0226, -0.0245],\n",
            "          [ 0.0000, -0.0059,  0.0000,  0.0344]],\n",
            "\n",
            "         [[ 0.0081, -0.0124, -0.0248, -0.0155],\n",
            "          [ 0.0000,  0.0000, -0.0233,  0.0205],\n",
            "          [ 0.0137, -0.0025, -0.0276,  0.0249],\n",
            "          [ 0.0000, -0.0022, -0.0292,  0.0000]],\n",
            "\n",
            "         [[-0.0043,  0.0498,  0.0000,  0.0000],\n",
            "          [-0.0082, -0.0029, -0.0003, -0.0324],\n",
            "          [ 0.0000,  0.0000,  0.0341,  0.0000],\n",
            "          [ 0.0000,  0.0000, -0.0364, -0.0047]],\n",
            "\n",
            "         [[ 0.0007,  0.0292,  0.0000,  0.0093],\n",
            "          [ 0.0048,  0.0017,  0.0088, -0.0248],\n",
            "          [-0.0177, -0.0109,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0149,  0.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0404,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000, -0.0309],\n",
            "          [ 0.0000,  0.0000,  0.0002,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0289,  0.0098,  0.0000,  0.0591],\n",
            "          [-0.0502, -0.0045, -0.0107, -0.0022],\n",
            "          [ 0.0000,  0.0000,  0.0193,  0.0196],\n",
            "          [-0.0225,  0.0000, -0.0279, -0.0061]],\n",
            "\n",
            "         [[ 0.0000,  0.0077,  0.0264,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0391,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000, -0.0140]]]])\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### REFERENCE\n",
        "\n",
        "https://web.stanford.edu/~nanbhas/blog/forward-hooks-pytorch/\n",
        "\n",
        "\n",
        "https://blog.paperspace.com/pytorch-hooks-gradient-clipping-debugging/\n",
        "\n",
        "https://medium.com/the-dl/how-to-use-pytorch-hooks-5041d777f904"
      ],
      "metadata": {
        "id": "pkgkjSw_yzM-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M2VjRehyy2db"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}